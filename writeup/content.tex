\section{Introduction}

Particle accelerators are very complex machines. Controlling these machines to achieve a beam of desired properties has proven to be a difficult task that takes expert human operators hours or even up to days or weeks to achieve. In some cases achieving certain beam parameters turns out to be impossible in practice despite being physically possible according to theory. This is the result of various difficult to predict effects such as inaccuracies and drifts that mean the real machine deviates from its physical models in ways substantial to their sensitive operation. Furthermore, some effects like drifts vary over time, making it impossible to return to optimal working modes even if one was found.

In most cases this problem is barely solved and most machines rely of experiences expert operators to regularly tune them to the desired working modes. Some solutions using black box optimisation have been implemented~\cite{TODO}. One specific such tool is Ocelot Optimiser~\cite{TODO}. These tools, however suffer from two downsides: Most black box optimisers converge towards local minima, which might lack far behind the desired performance. Such optimisers also tend to require hundreds of function evaluations to achieve convergence. On particle accelerators, setting magnets and then reading sensors can often take upwards of multiple seconds and in some cases even minutes, making some of these optimisation approaches prohibitively expensive due to the number of function evaluations that are required. The first issue can be approach by global optimisers like Bayesian Optimisation. But here too problems arise. Bayesian Optimisation requires choosing hyperparameters at run time and reacts very sensitively to choice of those in conjunction with the problem to optimise. It turns out that setting hyperparameters on-fly during operation only to discover that clearly that choice was not good and not know what might be a good choice drives operators away from using these algorithms and takes a lot of time in itself, even when it has been shown that there does exist a choice of the hyperparameters that has the algorithm solve the problem nicely.

Reinforcement learning promises to be a viable alternative to optimisation that can achieve competitive results using less function evaluations. At the same time \ac{RL} promises to remove the need for operators to have to find optimal hyperparameters and allow system designers to do so at design time. Furthermore, reinforcement learning has the potential to not only achieve optimal beam parameters, but also hold that while drift changes the machine or even while other parts of the machine are reconfigured. Unlike optimisation, \ac{RL} is likely to not have to make extreme steps away from the optimum, to find the latter, but rather has the ability to immediately take steps toward the optimum, improving safety and stability of machine operation.

\section{Reinforcement Learning}

\Acf{RL} is a machine learning technique to design and train agents capable of solving problems. A key advantage of \ac{RL} is that there is no need for labelled data, as agents are rewarded for the results they achieve and trained to maximise some cumulative reward over a so-called episode.

The key component in \ac{RL} is a function called the policy. The policy maps observations

\begin{itemize}
    \item Policies are denoted by either $\policydeterministic$ for deterministic policies or $\policystochastic$ for stochastic policies. A policy is a function that maps state $\bm{s}$ or observation $\observation$ to an action $\action$. It follows at time step $t$ that $\action_t = \pi(\bm{s}_t)$.
    \item It is important to differentiate between observations $\observation_t$ and states $\bm{s}_t$. An observation $\observation_t$ is the information that an agent observes about the environment at time step $t$. A state $\bm{s}_t$ is a complete description of the environment at time step $t$. In fully-observed environments $\observation_t = \bm{s}_t$ applies. Most environments however are partially observed and $\observation_t \neq \bm{s}_t$. \textbf{Be careful:} In reinforcement learning literature the symbol $\bm{s}_t$ is often used even when referring to an observation in a partially-observed environment. We may do the same when the context or accompanying definitions clarify this.
    \item While a policy can be an arbitrary function, many policies such as neural networks are parameterised by a set of parameters $\theta$. Parameterised policies are denoted using a subscript $\mu_\theta$ or $\pi_\theta$.
    \item Problems such as the ARES experimental area focusing and centring problem have an objective function $O(\actuators)$ of some input $\actuators$ describing how good or bad the inputs are. This is not to be confused with the value function $V(\bm{s}_t)$ or the action-value function $Q(\bm{s}_t, \action_t)$.
    \item The value function (otherwise known as state-value function) $V_{\pi_\theta}(\bm{s})$ gives the expected return when starting in state $\bm{s}$ and using policy $\pi_\theta$. This is not the same as the action-value function $Q_{\pi_\theta}(\bm{s}_t, \action_t)$, which gives the expected future return when executing action $\action_t$ in state $\bm{s}_t$ and then afterwards following policy $\pi_\theta$. According to Bellman's equations the on-policy action-value function $Q_{\pi_\theta}(\bm{s}_t, \action_t) = r(\bm{s}_t, \action_t) + \gamma V_{\pi_\theta}(\bm{s}_{t+1})$ or in terms of the action-value function $Q_{\pi_\theta}(\bm{s}_t, \action_t) = r(\bm{s}_t, \action_t) + \gamma Q_{\pi_\theta}(\bm{s}_{t+1}, \pi_\theta(s_{t+1}))$.
    \item Return $R$ is the cumulative reward from some time step $t$ forward. The reward $r_t$ for performing action $\action_t$ in state $\bm{s}_t$ is given by the reward function $r(\bm{s}_t, \action_t)$.
    \item The optimal policy, i.e. the policy that maximises the cumulative reward, is denoted by $\pi_*$. The optimal state- and action- value functions, i.e. the ground-truth value functions are denoted as $V_*$ and $Q_*$.
\end{itemize}

\section{Problem Formulation}

The electron accelerator ARES is a research and development machine at DESY. The layout of ARES is shown in \cref{fig:ares_layout}. We consider a section of ARES called Experimental Area 1. This section houses an experimental chamber where experiments require a well focussed and well positioned beam. A set of quadrupole and steerer magnets upstream from the experimental chamber can be used to produce the desired beam. The beam can be observed using a diagnostic screen in the experimental area. Focusing and positioning the beam by hand is a difficult and lengthy process that has to be repeated often and can take operators up to multiple hours to do. The results achieved by hand are not necessarily optimal.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{ares_layout}
    \caption{Layout of the charged particle accelerator ARES at DESY. The Experimental Area 1 considered in this work is found just left from the centre of the layout shown here.}
    \label{fig:ares_layout}
\end{figure}

We formulate this task as an optimisation problem where optimal focus and positioning is described in terms of the beam size in $x$- and $y$-direction $\beamwidth$ and $\beamheight$ as well as the beam position in $x$- and $y$-direction $\beampositionx$ and $\beampositiony$. The beam positions $\beampositionx$ and $\beampositiony$ are computed as the mean of all particles' positions in the respective dimension. Accordingly, the beam sizes $\beamwidth$ and $\beamheight$ are computed as the standard deviation of the particles' positions in the respective dimensions. We collect the beam parameters in a vector $\bm{b} = (\mu_x, \mu_y, \sigma_x, \sigma_y)$. All four beam parameters can be read from an image of the beam on the diagnostic screen. While there is a diagnostic screen in the experimental area, the latter is very small. We therefore consider a diagnostic screen downstream from the experimental area. In order to achieve focus and positioning, we can control a set of magnets $Q_1$, $Q_2$, $Q_3$, $C_v$ and $C_h$, where $Q_i$ is a quadrupole magnet, $C_v$ is a steerer magnet along the $y$-axis and $C_h$ is a steerer magnet along the $x$-axis. The inputs to these magnets are defined by the actuator vector $\actuators = (k_{Q_1}, k_{Q_2}, k_{Q_3}, \alpha_{C_v}, \alpha_{C_h})$, where $k_m$ is the strength of quadrupole $m$ measured in $\frac{1}{\text{m}^2}$, and $\alpha_m$ is the angle in $\unit{\radian}$ by which steerer $m$ deflects passing particles.

We define a black box function

\begin{equation}
    \bm{b} = f(\actuators)
\end{equation}

mapping an actuator vector $\actuators$ to resulting beam parameters $\actuators$ by setting the actuator values on the machine and reading the beam parameters from the diagnostic screen inside the machine. This constitutes our optimisation problem as

\begin{equation}
    \argmin_{\actuators} \; O(\actuators) = \sum_{p \in \bm{b}} \alpha_p |p|
\end{equation}

where $\alpha_p$ is a hyperparameter weighting for beam parameter $p$.

\section{Our Approach}

We can approach solving this black box optimisation problem using numerical black box optimisation algorithms, such as L-BFGS~\cite{TODO} or Bayesian Optimisation~\cite{TODO}. L-BFGS can find very good solutions most of the time and converge towards these in $300-800$ steps. \Cref{TODO} shows the final screen image of an L-BFGS optimisation as well as the development of actuator and beam parameter values over the steps taken. In some cases the L-BFGS algorithm does not converge toward an global optimum but gets stuck at its initial position instead. This likely occurs when that position is a local optimum. Furthermore, each step of setting the magnets and then reading the sensor values can take up to $10$ seconds on ARES. As a result the optimisation takes somewhere from $50$ minutes up to almost $2.5$ hours. This is about the same it takes a human operator to solve the same task. Yet one has to note that the result achieved by the numerical optimiser is better than that the human operator achieves, if the optimiser does in fact converge. A further disadvantage of black box optimisation is that algorithms like L-BFGS set extreme values for the actuators during optimisation. Such behaviour can be a risk to machine safety, resulting in machine protection intervention and beam loss in the best case, and in serious damage in the worst.

To resolve the aforementioned issues of black box optimisation in particle accelerator optimisation and control, be propose an approach based on \ac{RL}. Such approach promises potentially super-human performance and can, if necessary, be trained in a save simulation environment before every seeing the real machine.

To apply \ac{RL} to our optimisation problem, it must first be rephrased as an \ac{RL} problem. We define an action

\begin{equation}
    \action = (\Delta k_{Q_1}, \Delta k_{Q_2}, \Delta k_{Q_3}, \Delta \alpha_{C_v}, \Delta \alpha_{C_h})
\end{equation}

with $\Delta k_{Q_1}$ denoting the change to setting $k_{Q_1} \in \actuators$ and all other actuators respectively in this time step. Furthermore, we define an observation as

\begin{equation}
    \observation = (\mu_x, \mu_y, \sigma_x, \sigma_y, k_{Q_1}, k_{Q_2}, k_{Q_3}, \alpha_{C_v}, \alpha_{C_h})
\end{equation}

The problem is partially-observed. In simulation the \ac{RL} problem fulfils the Markov property. The reward is defined as

\begin{equation}
    r(\bm{s}_t, \action_t) = O(\actuators_{t-1}) - O(\actuators_t)
\end{equation}

where $\actuators_t = \actuators_0 + \sum_{i = 0}^t \action_i$. Intuitively, the reward $r_t$ is the improvement of the objective function at time step $t$ over the objective function at time step $t-1$. Figure \ref{fig:ares_ea_environment} gives an overview over the \ac{RL} problem. Note that the policy is only stochastic during training. It could (should) therefore be denoted as $\policydeterministic_\theta$.

\begin{figure}
    \centering
    \includegraphics[width=0.6\textwidth]{ares_ea_environment}
    \caption{Flow chart of the ARES experimental area reinforcement learning loop.}
    \label{fig:ares_ea_environment}
\end{figure}

To train the policy $\policydeterministic$, we use the TD3 algorithm~\cite{TODO} with the hyperparameters given in \cref{TODOTABLE}.

Training the \ac{RL} agent was not trivial. We identified crucial design decisions that had to be made to make \ac{RL} work well on this problem. First experiments indicate that these design decisions and general design patterns carry over to solving other tasks on particle accelerators using \ac{RL} as well. We list them in descending order of perceived importance.

Firstly, we scaled all action components, observation components and the reward. They should be scaled such that from the outside they fall somewhere into the interval $|[0.1,1]|$ in most situations. Different components of either actions or observation likely need to be scaled by different factors. This is to ensure that the optimisation problem is well defined in that all actuators are mapped to roughly the same range of values, which is unlikely to be that case for the true values. Furthermore, all components should be effectively normalised, hence the aforementioned interval. This is necessary to improved \ac{ANN} convergence and because the policy uses a $\text{tanh}$ activation function and will therefore only output values in the range $[-1,1]$. In our particular case we set sensible limits for all action and observation components and then apply normalisation according to those limits. The scalar for the reward is chosen manually.

Secondly, we chose to set actuators via deltas and include their absolute values in the observation. The reason for this design choice is twofold. Including the actuators' absolute values in the observation provides a lightweight form of temporal integration. Seeing both the current actuator settings and the resulting sensory observation components, allows the agent to deduce potential deviations of the machine from the ideal learned model of the latter. Setting deltas then instead of absolute actuator values, allows the agent to counter those deviations.

Thirdly, we chose a discount factor $\gamma = 0.55$. While this value is somewhat arbitrarily chosen, we found that discount factors below the value of $0.8$ perform much better than those above that threshold. We show the relation of the selected $\gamma$ and the achieved mean reward over episodes in \cref{fig:gamma_sweep}. This observation is unusual for \ac{RL} problems, where $\gamma$ is usually set on the interval $[0.9,1]$. It appears that the reason for this behaviour is TODO.

\begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth]{gamma_sweep}
    \caption{Lines connecting the chosen value for the discount factor $\gamma$ on the left to the achieved mean reward over episodes on the right. For $\gamma \leq 0.8$ the results are almost identical, with the mean reward dropping slightly for $\gamma = 0.9$ and the dropping significantly for $\gamma \geq 0.9$.}
    \label{fig:gamma_sweep}
\end{figure}
 
\begin{itemize}
    \item Let $f(\actuators)$ be named the accelerator function. It describes the particle movement through the beam line and the image they produce on the screen based on the settings $\actuators$ to the accelerator section's (considered) magnets. Let $f(\actuators) = \mathcal{S}(\mathcal{T}(P, \actuators))$.
    \item We define the screen acquisition function $\mathcal{S}(P) = (\mu_x(P), \mu_y(P), \sigma_x(P), \sigma_y(P))$ that retrieves a $4$-tuple of beam parameters from a set of particles $P$.
    \item The function $\mathcal{T}(P, \actuators) = PR_{\actuators}^T$ describes the transformations to particles $P$ when travelling through the beam line with actuator settings $\actuators$ applied. $R_{\actuators}$ is the transfer map describing the beam line under actuator settings $\actuators$ and computed as $R_{\actuators} = \prod_{E \in L} R_{\actuators}^{E}$ where $R_{\actuators}^{E}$ is the transfer map of a particular element such as a quadrupole magnet and $L$ is the lattice, a sequence of elements. One such sequence (not the exact sequence of the experimental area) may be defined as $L = (Q_1, D_1, Q_2, D_2, C_v, D_3, Q_3, D_4, C_h, D_5)$.
    \item The (true) value function of the optimal policy $V_{\mu_*}$ for the focusing and centring problem can be derived as $V_{\mu_*}(\observation) = O(\actuators)$ with $\observation = (\bm{b}, \actuators)$. That is $V_{\mu_*}$ is the expected improvement in the objective function when using the optimal policy. With the policy being optimal and deterministic, this is actually the maximum improvement. We know that the best possible objective function $O(\actuators_*) = 0$ and the best possible improvement is $V_{\mu_*}(\observation) = O(\actuators) - O(\actuators_*) = O(\actuators)$.
    \item The transfer maps $R_{\actuators}^{E}$ for different elements look as follows:
    \begin{itemize}
        \item Drift:
            \begin{equation*}
                D(l) = R_{\actuators}^D(l) = \begin{pmatrix}
                                             1 & l & 0 & 0 & 0 & 0 & 0 \\ % x
                                             0 & 1 & 0 & 0 & 0 & 0 & 0 \\ % xp
                                             0 & 0 & 1 & l & 0 & 0 & 0 \\ % y
                                             0 & 0 & 0 & 1 & 0 & 0 & 0 \\ % yp
                                             0 & 0 & 0 & 0 & 1 & \frac{l}{\gamma^2} & 0 \\ % delta s
                                             0 & 0 & 0 & 0 & 0 & 1 & 0 \\ % delta sp
                                             0 & 0 & 0 & 0 & 0 & 0 & 1 \\
                                         \end{pmatrix}
            \end{equation*}
        \item Focusing Quadrupole:
            \begin{equation*}
                Q(l,k) = R_{\actuators}^Q (l,k) = \begin{pmatrix}
                                                  \cos{\sqrt{k} l} & \frac{\sin{\sqrt{k} l}}{\sqrt{k}} & 0 & 0 & 0 & 0 & 0 \\ % x
                                                  -\sqrt{k}\sin{\sqrt{k} l} & \cos{\sqrt{k} l} & 0 & 0 & 0 & 0 & 0 \\ % xp
                                                  0 & 0 & \cosh{\sqrt{k} l} & \frac{\sinh{\sqrt{k} l}}{\sqrt{k}} & 0 & 0 & 0 \\ % y
                                                  0 & 0 & \sqrt{k}\sinh{\sqrt{k} l} & \cosh{\sqrt{k} l} & 0 & 0 & 0 \\ % yp
                                                  0 & 0 & 0 & 0 & 1 & \frac{l}{\gamma^2} & 0 \\ % delta s
                                                  0 & 0 & 0 & 0 & 0 & 1 & 0 \\ % delta sp
                                                  0 & 0 & 0 & 0 & 0 & 0 & 1 \\
                                              \end{pmatrix}
            \end{equation*}
        \item Defocusing Quadrupole:
            \begin{equation*}
                Q(l,k) = R_{\actuators}^Q (l,k) = \begin{pmatrix}
                                                  \cosh{\sqrt{k} l} & \frac{\sinh{\sqrt{k} l}}{\sqrt{k}} & 0 & 0 & 0 & 0 & 0 \\ % x
                                                  \sqrt{k}\sinh{\sqrt{k} l} & \cosh{\sqrt{k} l} & 0 & 0 & 0 & 0 & 0 \\ % xp
                                                  0 & 0 & \cos{\sqrt{k} l} & \frac{\sin{\sqrt{k} l}}{\sqrt{k}} & 0 & 0 & 0 \\ % y
                                                  0 & 0 & -\sqrt{k}\sin{\sqrt{k} l} & \cos{\sqrt{k} l} & 0 & 0 & 0  \\ % yp
                                                  0 & 0 & 0 & 0 & 1 & \frac{l}{\gamma^2} & 0 \\ % delta s
                                                  0 & 0 & 0 & 0 & 0 & 1 & 0 \\ % delta sp
                                                  0 & 0 & 0 & 0 & 0 & 0 & 1 \\
                                              \end{pmatrix}
            \end{equation*}
        \item Horizontal Corrector: 
            \begin{equation*}
                C_h(l,\alpha) = R_{\actuators}^{C_h}(l,\alpha) = \begin{pmatrix}
                                                                 1 & l & 0 & 0 & 0 & 0 & 0 \\ % x
                                                                 0 & 1 & 0 & 0 & 0 & 0 & \alpha \\ % xp
                                                                 0 & 0 & 1 & l & 0 & 0 & 0 \\ % y
                                                                 0 & 0 & 0 & 1 & 0 & 0 & 0 \\ % yp
                                                                 0 & 0 & 0 & 0 & 1 & \frac{l}{\gamma^2} & 0 \\ % delta s
                                                                 0 & 0 & 0 & 0 & 0 & 1 & 0 \\ % delta sp
                                                                 0 & 0 & 0 & 0 & 0 & 0 & 1 \\
                                                             \end{pmatrix}
            \end{equation*}
        \item Vertical Corrector:
            \begin{equation*}
                C_v(l,\alpha) = R_{\actuators}^{C_v}(l,\alpha) = \begin{pmatrix}
                                                                     1 & l & 0 & 0 & 0 & 0 & 0 \\ % x
                                                                     0 & 1 & 0 & 0 & 0 & 0 & 0 \\ % xp
                                                                     0 & 0 & 1 & l & 0 & 0 & 0 \\ % y
                                                                     0 & 0 & 0 & 1 & 0 & 0 & \alpha \\ % yp
                                                                     0 & 0 & 0 & 0 & 1 & \frac{l}{\gamma^2} & 0 \\ % delta s
                                                                     0 & 0 & 0 & 0 & 0 & 1 & 0 \\ % delta sp
                                                                     0 & 0 & 0 & 0 & 0 & 0 & 1 \\
                                                                 \end{pmatrix}
            \end{equation*}
    \end{itemize}
    \begin{itemize}
        \item $l$: length of the element in $\text{m}$
        \item $k$: quadrupole strength in $\frac{1}{\text{m}^2}$
        \item $\alpha$: corrector kick in $\text{rad}$
        \item $\gamma$: relativistic gamma  
    \end{itemize}
    
    \item Given the actuator vector $\actuators = (k_{Q_1}, k_{Q_2}, k_{Q_3}, \alpha_{C_v}, \alpha_{C_h})$ we have the (simplified but equivalent) lattice
    \begin{equation*}
    \begin{aligned}
        L = \langle & D_1(0.175), Q_1(0.122,k_{Q_1}), D_2(0.428), Q_2(0.122,k_{Q_2}), D_3(0.204), C_v(0.02,\alpha_{C_v}), \\ & D_4(0.204), Q_3(0.122,k_{Q_3}), D_5(0.179), C_h(0.02,\alpha_{C_h}), D_6(4.8791) \rangle
    \end{aligned}
    \end{equation*}
    
    % $$R_{\actuators} = Q1(Q1.k).D.Q2(Q2.k).D.Q3(Q3.k)$$
    
    \item Reformulation as ADP
    
    \begin{align*}
        \min_{x_t} \quad&\mathbb{E} V_t(x_t)\\ = \min_{x_t} \quad& \left(|f(x_t)|_1-|f(x_t+u_t)|_1 +\gamma \mathbb{E} V_{t+1}(x_{t+1}) \right)\\
        \text{such that} \quad &x_{t+1}=x_t+u_t\\
        & (x_t,u_t)\in\mathcal{Z}_t 
    \end{align*}
\end{itemize}

\section{Experiments}

After $\num{15000}$ to $\num{20000}$ steps, the training of the \ac{RL} agent converges to an agent that can solve the task well, though we observed that training longer for up to $\num{600000}$ steps results in an agent that performs more stably. TODO The resulting beam parameters achieved by the agent are similar to those achieved by numerical optimisers. The \ac{RL} agent however does not get stuck on certain initial conditions. Furthermore, the agent can reach the desired beam in $\num{5}$ to $\num{15}$ steps, i.e. less than $\num{2.5}$ minutes on the real machine. This is superhuman performance and negligible temporal cost.

\section{Conclusion}
