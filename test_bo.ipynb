{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing ocelot...\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from gym.wrappers import (\n",
    "    FilterObservation,\n",
    "    FlattenObservation,\n",
    "    FrameStack,\n",
    "    RecordVideo,\n",
    "    RescaleAction,\n",
    "    TimeLimit,\n",
    ")\n",
    "from stable_baselines3.common.env_util import unwrap_wrapper\n",
    "\n",
    "from bayesopt import calculate_objective, get_next_samples, scale_action\n",
    "from ea_optimize import (\n",
    "    ARESEADOOCS,\n",
    "    CallbackList,\n",
    "    OptimizeFunctionCallback,\n",
    "    report_ea_optimization_to_logbook,\n",
    ")\n",
    "from utils import (\n",
    "    FilterAction,\n",
    "    NotVecNormalize,\n",
    "    PolishedDonkeyCompatibility,\n",
    "    RecordEpisode,\n",
    "    send_to_elog,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble\n",
    "\n",
    "Remarks: I would set a higher max_step for BO, maybe 100/150?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = config = {\n",
    "    \"action_mode\": \"direct\",\n",
    "    \"gamma\": 0.99,\n",
    "    # \"filter_action\": [0, 1, 3],\n",
    "    \"filter_action\": None,\n",
    "    \"filter_observation\": None,\n",
    "    \"frame_stack\": None,\n",
    "    \"incoming_mode\": \"random\",\n",
    "    \"incoming_values\": None,\n",
    "    \"magnet_init_mode\": \"constant\",\n",
    "    \"magnet_init_values\": np.array([10, -10, 0, 10, 0]),\n",
    "    \"misalignment_mode\": \"constant\",\n",
    "    \"misalignment_values\": np.zeros(8),\n",
    "    \"n_envs\": 40,\n",
    "    \"normalize_observation\": True,\n",
    "    \"normalize_reward\": True,\n",
    "    \"rescale_action\": (-3, 3),\n",
    "    \"reward_mode\": \"logl1\",\n",
    "    \"sb3_device\": \"auto\",\n",
    "    \"target_beam_mode\": \"constant\",\n",
    "    \"target_beam_values\": np.zeros(4),\n",
    "    \"target_mu_x_threshold\": 1e-5,\n",
    "    \"target_mu_y_threshold\": 1e-5,\n",
    "    \"target_sigma_x_threshold\": 1e-5,\n",
    "    \"target_sigma_y_threshold\": 1e-5,\n",
    "    \"threshold_hold\": 5,\n",
    "    \"time_limit\": 50000,\n",
    "    \"vec_env\": \"subproc\",\n",
    "    \"w_done\": 0.0,\n",
    "    \"w_mu_x\": 1.0,\n",
    "    \"w_mu_x_in_threshold\": 0.0,\n",
    "    \"w_mu_y\": 1.0,\n",
    "    \"w_mu_y_in_threshold\": 0.0,\n",
    "    \"w_on_screen\": 0.0,\n",
    "    \"w_sigma_x\": 1.0,\n",
    "    \"w_sigma_x_in_threshold\": 0.0,\n",
    "    \"w_sigma_y\": 1.0,\n",
    "    \"w_sigma_y_in_threshold\": 0.0,\n",
    "    \"w_time\": 0.0,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a similar optimize function as in ea_optimize.py\n",
    "def optimize(\n",
    "    target_mu_x,\n",
    "    target_sigma_x,\n",
    "    target_mu_y,\n",
    "    target_sigma_y,\n",
    "    target_mu_x_threshold=3.3198e-6,\n",
    "    target_mu_y_threshold=3.3198e-6,\n",
    "    target_sigma_x_threshold=3.3198e-6,\n",
    "    target_sigma_y_threshold=3.3198e-6,\n",
    "    max_steps=100,\n",
    "    model_name=\"BO\",\n",
    "    logbook=False,\n",
    "    callback=None,\n",
    "    obj_function=\"logmae\",\n",
    "    acquisition=\"EI\",\n",
    "    init_x=None,\n",
    "    init_samples=5,\n",
    "    filter_action=None,\n",
    "):\n",
    "    # Create the environment\n",
    "    env = ARESEADOOCS(\n",
    "        action_mode=config[\"action_mode\"],\n",
    "        magnet_init_mode=config[\"magnet_init_mode\"],\n",
    "        magnet_init_values=config[\"magnet_init_values\"],\n",
    "        reward_mode=config[\"reward_mode\"],\n",
    "        target_beam_mode=config[\"target_beam_mode\"],\n",
    "        target_beam_values=np.array(\n",
    "            [target_mu_x, target_sigma_x, target_mu_y, target_sigma_y]\n",
    "        ),\n",
    "        target_mu_x_threshold=target_mu_x_threshold,\n",
    "        target_mu_y_threshold=target_mu_y_threshold,\n",
    "        target_sigma_x_threshold=target_sigma_x_threshold,\n",
    "        target_sigma_y_threshold=target_sigma_y_threshold,\n",
    "        threshold_hold=1,\n",
    "        w_done=config[\"w_done\"],\n",
    "        w_mu_x=config[\"w_mu_x\"],\n",
    "        w_mu_x_in_threshold=config[\"w_mu_x_in_threshold\"],\n",
    "        w_mu_y=config[\"w_mu_y\"],\n",
    "        w_mu_y_in_threshold=config[\"w_mu_y_in_threshold\"],\n",
    "        w_on_screen=config[\"w_on_screen\"],\n",
    "        w_sigma_x=config[\"w_sigma_x\"],\n",
    "        w_sigma_x_in_threshold=config[\"w_sigma_x_in_threshold\"],\n",
    "        w_sigma_y=config[\"w_sigma_y\"],\n",
    "        w_sigma_y_in_threshold=config[\"w_sigma_y_in_threshold\"],\n",
    "        w_time=config[\"w_time\"],\n",
    "    )\n",
    "    if callback is not None:\n",
    "        env = OptimizeFunctionCallback(env, callback)\n",
    "    env = RecordEpisode(env)\n",
    "    if config[\"filter_observation\"] is not None:\n",
    "        env = FilterObservation(env, config[\"filter_observation\"])\n",
    "    if config[\"filter_action\"] is not None:\n",
    "        env = FilterAction(env, config[\"filter_action\"], replace=0)\n",
    "    env = FlattenObservation(env)\n",
    "    if config[\"frame_stack\"] is not None:\n",
    "        env = FrameStack(env, config[\"frame_stack\"])\n",
    "    if config[\"rescale_action\"] is not None:\n",
    "        env = RescaleAction(\n",
    "            env, config[\"rescale_action\"][0], config[\"rescale_action\"][1]\n",
    "        )\n",
    "    env = RecordVideo(env, video_folder=f\"recordings_real/{datetime.now():%Y%m%d%H%M}\")\n",
    "    # env = NotVecNormalize(env, f\"models/{model_name}/vec_normalize.pkl\")\n",
    "\n",
    "    callback = CallbackList(callback) if isinstance(callback, list) else callback\n",
    "\n",
    "    # Actual optimisation\n",
    "    t_start = datetime.now()\n",
    "    observation = env.reset()\n",
    "    beam_image_before = env.get_beam_image()\n",
    "    done = False\n",
    "\n",
    "    # Initialization\n",
    "    x_dim = env.action_space.shape[0]\n",
    "    bounds = torch.tensor(\n",
    "        np.array([env.action_space.low, env.action_space.high]), dtype=torch.float32\n",
    "    )\n",
    "    if init_x is not None:  # From fix starting points\n",
    "        X = torch.tensor(init_x.reshape(-1, x_dim), dtype=torch.float32)\n",
    "    else:  # Random Initialization\n",
    "        action_i = scale_action(env, observation, filter_action)\n",
    "        X = torch.tensor([action_i], dtype=torch.float32)\n",
    "        for i in range(init_samples - 1):\n",
    "            X = torch.cat([X, torch.tensor([env.action_space.sample()])])\n",
    "    # Sample initial Ys to build GP\n",
    "    Y = torch.empty((X.shape[0], 1))\n",
    "    for i, action in enumerate(X):\n",
    "        action = action.detach().numpy()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        objective = calculate_objective(env, observation, reward, obj=obj_function)\n",
    "        Y[i] = torch.tensor(objective)\n",
    "\n",
    "    # Actual BO Loop\n",
    "    while not done:\n",
    "        # action, _ = model.predict(observation, deterministic=True)\n",
    "        action_t = get_next_samples(\n",
    "            X, Y, Y.max(), bounds, n_points=1, acquisition=acquisition\n",
    "        )\n",
    "        action = action_t.detach().numpy().flatten()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        objective = calculate_objective(env, observation, reward, obj=obj_function)\n",
    "\n",
    "        # append data\n",
    "        X = torch.cat([X, action_t])\n",
    "        Y = torch.cat([Y, torch.tensor([[objective]], dtype=torch.float32)])\n",
    "\n",
    "    t_end = datetime.now()\n",
    "\n",
    "    recording = unwrap_wrapper(env, RecordEpisode)\n",
    "    if logbook:\n",
    "        report_ea_optimization_to_logbook(\n",
    "            model_name,\n",
    "            t_start,\n",
    "            t_end,\n",
    "            recording.observations,\n",
    "            recording.infos,\n",
    "            beam_image_before,\n",
    "            target_mu_x_threshold,\n",
    "            target_sigma_x_threshold,\n",
    "            target_mu_y_threshold,\n",
    "            target_sigma_y_threshold,\n",
    "        )\n",
    "\n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual BO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z8/vzg_1dr50gg1zchydp1styc00000gn/T/ipykernel_43755/3389021298.py:84: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:204.)\n",
      "  X = torch.cat([X, torch.tensor([env.action_space.sample()])])\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/rl39/lib/python3.9/site-packages/botorch/optim/optimize.py:287: RuntimeWarning: Optimization failed in `gen_candidates_scipy` with the following warning(s):\n",
      "[OptimizationWarning('Optimization failed within `scipy.optimize.minimize` with status 2.')]\n",
      "Trying again with a new set of initial conditions.\n",
      "  warnings.warn(first_warn_msg, RuntimeWarning)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/rl39/lib/python3.9/site-packages/botorch/optim/optimize.py:287: RuntimeWarning: Optimization failed in `gen_candidates_scipy` with the following warning(s):\n",
      "[OptimizationWarning('Optimization failed within `scipy.optimize.minimize` with status 1.')]\n",
      "Trying again with a new set of initial conditions.\n",
      "  warnings.warn(first_warn_msg, RuntimeWarning)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/rl39/lib/python3.9/site-packages/botorch/optim/optimize.py:298: RuntimeWarning: Optimization failed on the second try, after generating a new set of initial conditions.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/rl39/lib/python3.9/site-packages/botorch/optim/optimize.py:287: RuntimeWarning: Optimization failed in `gen_candidates_scipy` with the following warning(s):\n",
      "[OptimizationWarning('Optimization failed within `scipy.optimize.minimize` with status 1.')]\n",
      "Trying again with a new set of initial conditions.\n",
      "  warnings.warn(first_warn_msg, RuntimeWarning)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/rl39/lib/python3.9/site-packages/botorch/optim/optimize.py:287: RuntimeWarning: Optimization failed in `gen_candidates_scipy` with the following warning(s):\n",
      "[OptimizationWarning('Optimization failed within `scipy.optimize.minimize` with status 2.')]\n",
      "Trying again with a new set of initial conditions.\n",
      "  warnings.warn(first_warn_msg, RuntimeWarning)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# hopefully this would run :)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_mu_x\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.4249e-3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_sigma_x\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_mu_y\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.1048e-3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_sigma_y\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_mu_x_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3.16e-6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_mu_y_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3.16e-6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_sigma_x_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3.16e-6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_sigma_y_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3.16e-6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBO\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogbook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogmae\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43macquisition\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEI\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_x\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilter_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [3], line 100\u001b[0m, in \u001b[0;36moptimize\u001b[0;34m(target_mu_x, target_sigma_x, target_mu_y, target_sigma_y, target_mu_x_threshold, target_mu_y_threshold, target_sigma_x_threshold, target_sigma_y_threshold, max_steps, model_name, logbook, callback, obj_function, acquisition, init_x, init_samples, filter_action)\u001b[0m\n\u001b[1;32m     96\u001b[0m action_t \u001b[38;5;241m=\u001b[39m get_next_samples(\n\u001b[1;32m     97\u001b[0m     X, Y, Y\u001b[38;5;241m.\u001b[39mmax(), bounds, n_points\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, acquisition\u001b[38;5;241m=\u001b[39macquisition\n\u001b[1;32m     98\u001b[0m )\n\u001b[1;32m     99\u001b[0m action \u001b[38;5;241m=\u001b[39m action_t\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m--> 100\u001b[0m observation, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m objective \u001b[38;5;241m=\u001b[39m calculate_objective(env, observation, reward, obj\u001b[38;5;241m=\u001b[39mobj_function)\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# append data\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/rl39/lib/python3.9/site-packages/gym/wrappers/record_video.py:86\u001b[0m, in \u001b[0;36mRecordVideo.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m---> 86\u001b[0m     observations, rewards, dones, infos \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m(RecordVideo, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     88\u001b[0m     \u001b[39m# increment steps and episodes\u001b[39;00m\n\u001b[1;32m     89\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_id \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/rl39/lib/python3.9/site-packages/gym/core.py:289\u001b[0m, in \u001b[0;36mWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m--> 289\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/rl39/lib/python3.9/site-packages/gym/core.py:349\u001b[0m, in \u001b[0;36mActionWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m--> 349\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maction(action))\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/rl39/lib/python3.9/site-packages/gym/core.py:323\u001b[0m, in \u001b[0;36mObservationWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m--> 323\u001b[0m     observation, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m    324\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation(observation), reward, done, info\n",
      "File \u001b[0;32m~/Documents/DESY/ares-ea-rl/utils.py:180\u001b[0m, in \u001b[0;36mRecordEpisode.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m--> 180\u001b[0m     observation, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m    182\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservations\u001b[39m.\u001b[39mappend(observation)\n\u001b[1;32m    183\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrewards\u001b[39m.\u001b[39mappend(reward)\n",
      "File \u001b[0;32m~/Documents/DESY/ares-ea-rl/ea_train.py:339\u001b[0m, in \u001b[0;36mARESEA.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m    337\u001b[0m     \u001b[39m# Perform action\u001b[39;00m\n\u001b[1;32m    338\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdirect\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 339\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mset_magnets(action)\n\u001b[1;32m    340\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdirect_unidirectional_quads\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    341\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_magnets(action)\n",
      "File \u001b[0;32m~/Documents/DESY/ares-ea-rl/ea_optimize.py:302\u001b[0m, in \u001b[0;36mARESEADOOCS.set_magnets\u001b[0;34m(self, magnets)\u001b[0m\n\u001b[1;32m    296\u001b[0m pydoocs\u001b[39m.\u001b[39mwrite(\n\u001b[1;32m    297\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mSINBAD.MAGNETS/MAGNET.ML/AREAMCHM1/KICK_MRAD.SP\u001b[39m\u001b[39m\"\u001b[39m, magnets[\u001b[39m4\u001b[39m] \u001b[39m*\u001b[39m \u001b[39m1000\u001b[39m\n\u001b[1;32m    298\u001b[0m )\n\u001b[1;32m    300\u001b[0m \u001b[39m# Wait until magnets have reached their setpoints\u001b[39;00m\n\u001b[0;32m--> 302\u001b[0m time\u001b[39m.\u001b[39;49msleep(\u001b[39m3.0\u001b[39;49m)  \u001b[39m# Wait for magnets to realise they received a command\u001b[39;00m\n\u001b[1;32m    304\u001b[0m magnets \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mAREAMQZM1\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mAREAMQZM2\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mAREAMCVM1\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mAREAMQZM3\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mAREAMCHM1\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    306\u001b[0m are_busy \u001b[39m=\u001b[39m [\u001b[39mTrue\u001b[39;00m] \u001b[39m*\u001b[39m \u001b[39m5\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# hopefully this would run :)\n",
    "\n",
    "optimize(\n",
    "    target_mu_x=0.4249e-3,\n",
    "    target_sigma_x=0.0,\n",
    "    target_mu_y=1.1048e-3,\n",
    "    target_sigma_y=0.0,\n",
    "    target_mu_x_threshold=3.16e-6,\n",
    "    target_mu_y_threshold=3.16e-6,\n",
    "    target_sigma_x_threshold=3.16e-6,\n",
    "    target_sigma_y_threshold=3.16e-6,\n",
    "    max_steps=100,\n",
    "    model_name=\"BO\",\n",
    "    logbook=True,\n",
    "    callback=None,\n",
    "    obj_function=\"logmae\",\n",
    "    acquisition=\"EI\",\n",
    "    init_x=None,\n",
    "    init_samples=5,\n",
    "    filter_action=None,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('rl39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "343fe3b89e2d7877d61a0509fd880204236e5c07449e4c121f53f2530ef83fc9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
